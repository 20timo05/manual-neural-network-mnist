{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D2J-OaAmO4yw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xiU-TggfPH6e"
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "INPUT_NODES = 28 * 28\n",
    "OUTPUT_NODES = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeoZs4dAPIT7",
    "outputId": "3f11b530-aeea-4631-bb40-42ad21c921ac"
   },
   "outputs": [],
   "source": [
    "training_data = MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "a63KHLBlPZd5"
   },
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, shape):\n",
    "        # initialize weights with xavier initialization\n",
    "        self.weights = torch.randn(shape) * torch.sqrt(torch.tensor(2 / (shape[0] + shape[1])))\n",
    "        self.bias = torch.randn(shape[1])\n",
    "        \n",
    "    def __call__(self, input):\n",
    "        self.input = input\n",
    "        out = input @ self.weights + self.bias\n",
    "        return out\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        # --- comments are an example for linear layer with 100 input, 10 output nodes & BATCH_SIZE = 32 ---\n",
    "        \n",
    "        # doutput.shape [32, 1, 10]\n",
    "        # input.shape [32, 1, 100]  => .T [100, 1, 32]\n",
    "        # weights.shape [100, 10]\n",
    "        self.weights.grad = self.input.squeeze(1).T @ doutput.squeeze(1) # [100, 32] @ [32, 10] = [100, 10]\n",
    "        self.bias.grad = doutput.sum(0).squeeze(0) # doutput.shape [32, 1, 10] => .sum(0).squeeze(0) [10]\n",
    "        \n",
    "        dinput = doutput @ self.weights.T # [32, 1, 10] @ [10, 100] = [32, 1, 100]\n",
    "        return dinput\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights, self.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "Q5B8I5jlPaUd"
   },
   "outputs": [],
   "source": [
    "class TanH:\n",
    "    def __call__(self, input):\n",
    "        self.output = (torch.exp(input) - torch.exp(-input)) / (torch.exp(input) + torch.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, doutput):\n",
    "        # d/dx (tanh(x)) = 1 - tanhÂ²(x)\n",
    "        dinput = (1 - self.output**2) * doutput\n",
    "        return dinput\n",
    "        \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "id": "YOWDNfCfPJsH"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            # (batch_size, 1, 784)\n",
    "            LinearLayer((INPUT_NODES, 100)), # (batch_size, 1, hidden_nodes)\n",
    "            TanH(),\n",
    "            LinearLayer((100, OUTPUT_NODES)) # (batch_size, 1, 10)\n",
    "        ]\n",
    "\n",
    "        self.parameters = [p for layer in self.layers for p in layer.parameters()]\n",
    "        \n",
    "        for parameter in self.parameters:\n",
    "            parameter.requires_grad = True\n",
    "            parameter.retain_grad()\n",
    "\n",
    "    def __call__(self, input):\n",
    "        output = input\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return self.softmax(output)\n",
    "\n",
    "    def train(self, epochs, train_data):\n",
    "        print(\"------- START TRAINING -------\\n\")\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_losses = []\n",
    "\n",
    "            for batch in train_loader:\n",
    "                images, labels = batch\n",
    "\n",
    "                # flatten imgs so that they can be passed to NN\n",
    "                flattened_imgs = images.flatten(-2) # (batch_size, 1, 28, 28) => (batch_size, 1, 784)\n",
    "\n",
    "                # get predictions from NN\n",
    "                preds = flattened_imgs\n",
    "                for layer in self.layers:\n",
    "                    preds = layer(preds)\n",
    "\n",
    "                # get loss for each output node\n",
    "                loss, dlogits = self.cross_entropy_loss(preds.squeeze(1), labels)\n",
    "                dlogits.retain_grad()\n",
    "                \n",
    "                # manual backprop through layers of model\n",
    "                doutput = dlogits.unsqueeze(1)\n",
    "                for layer in reversed(self.layers):\n",
    "                    doutput = layer.backward(doutput)\n",
    "                    \n",
    "                # loss.backward()\n",
    "                \n",
    "                # update weights based on gradients\n",
    "                for layer in self.layers:\n",
    "                    for parameter in layer.parameters():\n",
    "                        parameter.data = parameter.data - parameter.grad * LEARNING_RATE\n",
    "                        parameter.grad.zero_()\n",
    "\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "            epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "            print(f\"Epoch {epoch + 1} / {epochs}: tr_loss: {epoch_loss}\")\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        # Apply softmax to the predictions \n",
    "        norm_logits = logits - torch.max(logits, dim=1, keepdim=True)[0] # subtract max for numerical stability\n",
    "        soft_preds = norm_logits.exp() / norm_logits.exp().sum(1, keepdim=True) # softmax formula\n",
    "        \n",
    "        return soft_preds\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        # predictions.shape => [batch_size, 1, 10]\n",
    "        # labels.shape => [batch_size]\n",
    "        soft_preds = self.softmax(logits)\n",
    "        log_preds = soft_preds.log()\n",
    "        loss = -log_preds[range(BATCH_SIZE), labels].mean()\n",
    "        \n",
    "        # manual backprop\n",
    "        dlogits = soft_preds.clone()\n",
    "        dlogits[range(BATCH_SIZE), labels] -= 1\n",
    "        dlogits /= BATCH_SIZE\n",
    "        \n",
    "        return loss, dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "RlTU8zzDQATg",
    "outputId": "a4c5eee8-92c8-4ad6-bd10-457cd8d363ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- START TRAINING -------\n",
      "\n",
      "Epoch 1 / 5: tr_loss: 2.5868744150797527\n",
      "Epoch 2 / 5: tr_loss: 2.511892426172892\n",
      "Epoch 3 / 5: tr_loss: 2.4491195699055988\n",
      "Epoch 4 / 5: tr_loss: 2.3963043631871543\n",
      "Epoch 5 / 5: tr_loss: 2.3515102249145508\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.train(5, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((100, 1, 32)).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
